---
title: 'basics on learning brain cognitive correlates'
date: 2019-03-03
permalink: /posts/2019/03/basics-on-learning-brain-cognitive-correlates/
tags:
  - machine learning
  - neuroimaging
---

In this post I explain how to go about when trying to find potential determinants for cognition among high-dimensional data such as that from brain structure.
I show that techniques used when few determinants such as age and sex are no longer useful.
Finally, I show how machine learning provides an explanation for this issue and proposes a solution.


Cognitive abilities can be measured across multiple domains such as _working memory_, _processing speed_, _episodic verbal memory_ and _executive function_.
One may wonder how they are determined by factors such as age and sex.
Similarly, one may also want to know how much left is determined by brain structure.
To answer these questions one may use regression analysis to assess the association between cognition and these factors or, put another way, to predict cognition using these factors as input.

To get an idea of the data, here we plot different cognitive abilities with respect to age for a subset of participants in the [Rhineland Study](https://www.rheinland-studie.de/) (variables have been standardized to zero-norm, unit standard deviation).

![](/images/blog/2019-03-03-basics-on-learning-brain-cognitive-correlates/plot_cog_age.png)

One may define a function that predicts cognition based on age as input.

![](/images/blog/2019-03-03-basics-on-learning-brain-cognitive-correlates/fn_cog_age.png)

(we are assuming a linear relation between cognition and age)

In regression analysis, we need a set of examples with pairs of values for age (![](/images/blog/2019-03-03-basics-on-learning-brain-cognitive-correlates/x.png)) and cognition (![](/images/blog/2019-03-03-basics-on-learning-brain-cognitive-correlates/y.png)). 
We estimate the beta coefficients that make up the function as follows:

![](/images/blog/2019-03-03-basics-on-learning-brain-cognitive-correlates/fn_opt.png)

That is, we seek the function _f()_ that predicts with minimum error (ie, squared difference) the cognitive abilities _y_ in our sample.
The predictions generated by such a function are plotted in red below.
Errors are represented as black lines joining predictions with true values.
(we actually compute 4 different functions, one for each cognitive domain)

![](/images/blog/2019-03-03-basics-on-learning-brain-cognitive-correlates/plot_cog_age_pred1.png)

We may extract conclusions about the importance of age in predicting cognition.
We won't go into more detail into that here.
Let us only note that in general, the greater the magnitude of the beta coefficient, the more important is the associated factor in the prediction.

To further improve prediction, we may proceed by including other factors such as sex.

![](/images/blog/2019-03-03-basics-on-learning-brain-cognitive-correlates/plot_cog_age_pred2.png)

We see that adding sex brings the predictions a bit closer to the true values.

If available, information about the brain structure is another obvious candidate that may improve the predictions.
Luckily enough, we have information about thickness across 50+ cortical brain structures in our sample.

Below we show the evolution of the prediction by gradually adding all the available variables, starting by age and sex and continuing with the cortical thickness measures.

![](/images/blog/2019-03-03-basics-on-learning-brain-cognitive-correlates/lars_anim.gif)

As we include more variables the fit gets better until eventually we obtain perfect prediction in our sample.

At this point we have created 4 functions _f()_ (one for each cognitive domain) that receive age, sex and cortical thickness as input and predict cognitive performance as output.
Does this mean that we now exactly predict the cognitive abilities of any person given their age, sex and cortical measures ?
**Of course not**. 
As a counter-example, consider the how unlikely it is that one participant from our sample would perform exactly the same if they were to repeat the cognitive testing.
However, our function would always predict the same value, as long as age, sex and cortical thickness remain constant.

Let us make clear the distinction between **learning** and **prediction**.
- Learning involves the creation of the functions relating biological inputs to cognition using an example set of _training_ data
- Predicting means guessing the cognitive performance from the biological inputs, using the functions learned in the previous step

Note that prediction can be done in _the same_ or _another_ dataset used for learning the functions.
To see how well the learned functions capture the relationships between the biological inputs and cognitive performance, let us apply them to a different set of participants.

![](/images/blog/2019-03-03-basics-on-learning-brain-cognitive-correlates/plot_oos.png)

The errors are so large that do not even fit in the plot.
** What has happened ?**
With so many inputs, the function is flexible enough to behave in any way we want.
As result, it only learns relationships between inputs and output specific to the training data but that do not hold to other participants.
In machine learning terms, this is known as [overfitting](https://en.wikipedia.org/wiki/Overfitting).

**Therefore, to ascertain that functions with high-dimensional inputs (>10-12) capture any relationship beyond the training dataset, we always need to test for generalization.**
When using only a few variables the function is already constrained enough to be protected against over-fitting, although testing for generalization would never harm.

To understand the issue consider the following.
The true relationship between input and output (if any) hold in any representative sub-set of the data.
Therefore, the learning procedure that reveals such true relationship should yield stable models regardless of the training dataset.
Highly flexible models contradict this premise by yielding results that depend too much on the particular training set.
Note also that too stable models (eg, a constant function) would also fail for the opposite reason, ie, not explaining anything useful about the data.
Therefore, we seek a trade-off between adaptability to the data and stability of the model across training sets.
This is known as the [bias-variance tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff), a central issue in machine learning theory.

The bias-variance trade-off suggests that we should sacrifice data fitting for the sake of a more stable model across training sets.
Although it may not be obvious how allowing for training errors improves generalization, I would argue it is otherwise unreasonable to pursue a perfect fitting for the following reasons:
- missing inputs: we may be missing relevant biological factors that determine cognitive performance
- wrong model assumptions: the relationships between the inputs and outputs in our model (linear in our example) are not correct
- random errors: the same person may perform differently in the same test for reasons that may not be determined

Now that I hope to have convinced you, you may ask how we deliberately control the degree of flexibility of our function and how to select the appropriate degree ?
The first question relates to the topic of [regularization](https://en.wikipedia.org/wiki/Regularization_(mathematics)).
Best-feature-subset selection methods such as the [lasso](https://en.wikipedia.org/wiki/Lasso_(statistics)) go about this issue by restricting the model to only use a reduced set of inputs instead of all.

Regarding the second question on how to decide on the appropriate degree of flexibility, this is the topic of [model selection](https://en.wikipedia.org/wiki/Model_selection).
One of the most common methods is [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)), which consists in helding-out part of the data for measuring generalizability.
 
I describe in the following how to go about doing this in our application of predicting cognitive abilities:
- Partition the dataset in 3 parts for training, validation and test.
- Using the training set, we learn different versions of the model, by varying the degree of complexity.
- Using the validation set, we measure prediction accuracy, ie, how well the model learned on the training data predicts cognition on the validation set
- We select the complexity level with best prediction accuracy on the validation set and re-learn the model using both training and validation sets

We can now use the learned model to predict the cognitive performance on the data from the test set, which has not been used at all and which represents new data that may come in the future.
The image below shows the prediction on a test set using age, sex and all brain variables.

![](/images/blog/2019-03-03-basics-on-learning-brain-cognitive-correlates/plot_lasso.png)

Now that we have made sure that we our does not overfit the data, we can obtain conclusions about the brain structures that are important for predicting cognition.
Overlaid in colors, the image below shows the brain structures selected by lasso, where blue / red denote respectively that the  thickening / thinning of the corresponding structure is associated with higher cognitive performance.


![](/images/blog/2019-03-03-basics-on-learning-brain-cognitive-correlates/ef_dorsal.png)
![](/images/blog/2019-03-03-basics-on-learning-brain-cognitive-correlates/evm_dorsal.png)
![](/images/blog/2019-03-03-basics-on-learning-brain-cognitive-correlates/ps_dorsal.png)
![](/images/blog/2019-03-03-basics-on-learning-brain-cognitive-correlates/wm_dorsal.png)

