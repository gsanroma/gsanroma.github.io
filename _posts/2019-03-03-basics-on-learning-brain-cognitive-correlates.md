---
title: 'basics on learning brain cognitive correlates'
date: 2019-03-03
permalink: /posts/2019/03/basics-on-learning-brain-cognitive-correlates/
tags:
  - machine learning
  - neuroimaging
---

In this post I show how to find brain structures that are associated with cognitive abilities independently of factors such as age and sex.
In the process of solving this problem with standard statistical techniques we come across an issue related to the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality).
Finally, I show how machine learning comes to hand to solve this issue.

Cognitive abilities can be measured across multiple domains such as _working memory_, _processing speed_, _episodic verbal memory_ and _executive function_.
Human abilities in such domains may be influenced by factors such as age and sex.
One may want to determine the importance of these factors on cognition and also how much left is determined by brain structure.
To answer the first question one may resort to regression analysis.

The image below shows plots of different cognitive abilities with respect to age for a subset of participants in the [Rhineland Study](https://www.rheinland-studie.de/) (variables have been standardized to zero-norm, unit standard deviation).

![](/images/blog/2019-03-03-basics-on-learning-brain-cognitive-correlates/plot_cog_age.png)

We can see that cognitive abilities generally decrease with age.
In order to see how much cognitive abilities are determined by age, we can define one function for each domain that predicts cognition only from age.
Making the plausible assumption that the relation is linear, we may write:

![](/images/blog/2019-03-03-basics-on-learning-brain-cognitive-correlates/fn_cog_age.png)

This is a function to predict cognition using as input _x_, the age for a given subject.
We estimate the beta coefficients making up this function by using regression analysis as follows:

![](/images/blog/2019-03-03-basics-on-learning-brain-cognitive-correlates/fn_opt.png)

That is, we seek the function _f()_ that predicts more accurately (ie, with minimum squared difference) the cognitive abilities _y_ of our subjects (indexed with subscript _i_) using their age as input.

The plot below shows the predictions generated by such a function in red.
The errors for each participant are shown as a vertical black line.

![](/images/blog/2019-03-03-basics-on-learning-brain-cognitive-correlates/plot_cog_age_pred1.png)

As you can see, there's still a considerable variability not explained by age, but a rough prediction can already be done.
The magnitude of the coefficient is somehow related to the importance of the factor.

Our goal may not be to accurately predict age but to estimate the effect of different factors on cognition.
This can be done with statistical inference.
Let us assume here that our goal is to obtain an as accurate prediction as possible.
Prediction focuses in producing individualized estimates.
This is a useful goal for example in exploratory analysis when we have a lot of variables.

To improve the prediction, we can proceed as before by including other factors into our predicting function.
For example, if we include sex we obtain the following.

![](/images/blog/2019-03-03-basics-on-learning-brain-cognitive-correlates/plot_cog_age_pred2.png)

We obtain a bit better predictions.
We also can see the cognitive domains that sex helps predict cognitive domains in our sample.

To reliably ascertain the effect of sex on cognition we should need to adjust for factors that may be associated with sex in our sample and that are affecting cognition.
This is however out of this analysis, which is to build a function that predicts cognition in our sample, whatever is driving this association.

Next thing we may want to include are brain structural measurements such as the thickness of the cortex in across different regions.
This is an obvious candidate.
By having included age and sex in the function, the additional variability added by brain regions will be unrelated to these.
This is a design decision.
We may want to find the regions that are related to cognition mediated by age, but this is not the question we are asking here.
Here we want to find the factors that predict cognition.
Since we have included age and sex in the model, what we are interested in brain is whatever it explains that is not already explained by these.

Since there are lot of variables in the brain (>50), we show below an animation by including one variable at a time (starting by age and sex), to see how the variance is gradually explained.

![](/images/blog/2019-03-03-basics-on-learning-brain-cognitive-correlates/anim.gif)

As we include more variables the fit gets better until eventually the predictions in our sample are perfect.
We could now look at the coefficients to deduce importance of regions.

Does this mean that we can predict now perfectly the cognitive performance when given age, sex and cortical thickness of any individual.
Obviously, this is not true because even the same people may perform differently if repeating.
Therefore, we may not aspire at doing it perfect.

Let us see how the model fitted in the previous sample performs in another sample of people from the same population:

![](/images/blog/2019-03-03-basics-on-learning-brain-cognitive-correlates/plot_oos.png)

The errors are so large that do not even fit in the plot.
Therefore, here we cannot extract any conclusion that generalizes.
What has happened ?
With so many variables, we have allowed too much flexibility that the model adapts perfectly to the sample but tells nothing about the population.
This is known as [overfitting](https://en.wikipedia.org/wiki/Overfitting).

When using many variables, if we want to extract valid conclusions about data it needs to generalize.
With few variables this is not a problem and we may reach valid conclusions that generalize.
This is the approach taken when there is a clear hypothesis and when we have the right measurements to validate the hypothesis.
However, when we have a lot of variables and want to extract valid conclusions, prediction is one way to go.
We could also do multiple individual analyses, including only one variable (keeping confounders fixed throughout).
This is the approach taken in voxel-based morphometry (in neuroimage) or the famous [GWAS](https://en.wikipedia.org/wiki/Genome-wide_association_study).
In this way we may extract conclusions about each individual entity.
Prediction allows us to account for the joint effect

It is something obvious that we may not aspire to explain perfectly.
Because of random errors. 
Wrong model assumptions.
Factors not considered.

Machine learning theory explains this phenomenon as the [bias-variance tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff).
Basically 

