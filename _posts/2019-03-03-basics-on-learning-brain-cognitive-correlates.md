---
title: 'basics on learning brain cognitive correlates'
date: 2019-03-03
permalink: /posts/2019/03/basics-on-learning-brain-cognitive-correlates/
tags:
  - machine learning
  - neuroimaging
---

In this post I show how to find brain structures that are associated with cognitive abilities independently of factors such as age and sex.
In the process of solving this problem with standard statistical techniques we come across an issue related to the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality).
Finally, I show how machine learning comes to hand to solve this issue.

Cognitive abilities can be measured across multiple domains such as _working memory_, _processing speed_, _episodic verbal memory_ and _executive function_.
Human abilities in such domains may be influenced by factors such as age and sex.
One may want to determine the importance of these factors on cognition and also how much left is determined by brain structure.
To answer the first question one may resort to regression analysis.

The image below shows plots of different cognitive abilities with respect to age for a subset of participants in the [Rhineland Study](https://www.rheinland-studie.de/) (variables have been standardized to zero-norm, unit standard deviation).

![](/images/blog/2019-03-03-basics-on-learning-brain-cognitive-correlates/plot_cog_age.png)

We can see that cognitive abilities generally decrease with age.
In order to see how much cognitive abilities are determined by age, we can define one function for each domain that predicts cognition only from age.
Making the plausible assumption that the relation is linear, we may write:

![](/images/blog/2019-03-03-basics-on-learning-brain-cognitive-correlates/fn_cog_age.png)

This is a function to predict cognition using as input _x_, the age for a given subject.
We estimate the beta coefficients making up this function by using regression analysis as follows:

![](/images/blog/2019-03-03-basics-on-learning-brain-cognitive-correlates/fn_opt.png)

That is, we seek the function _f()_ that predicts more accurately (ie, with minimum squared difference) the cognitive abilities _y_ of our subjects (indexed with subscript _i_) using their age as input.

The plot below shows the predictions generated by such a function in red.
The errors for each participant are shown as a vertical black line.

![](/images/blog/2019-03-03-basics-on-learning-brain-cognitive-correlates/plot_cog_age_pred1.png)

As you can see, there's still a considerable variability not explained by age, but a rough prediction can already be done.
The magnitude of the coefficient is somehow related to the importance of the factor.

Our goal may not be to accurately predict age but to estimate the effect of different factors on cognition.
This can be done with statistical inference.
Let us assume here that our goal is to obtain an as accurate prediction as possible.
Prediction focuses in producing individualized estimates.
This is a useful goal for example in exploratory analysis when we have a lot of variables.

To improve the prediction, we can proceed as before by including other factors into our predicting function.
For example, if we include sex we obtain the following.

![](/images/blog/2019-03-03-basics-on-learning-brain-cognitive-correlates/plot_cog_age_pred2.png)

We obtain a bit better predictions.
We also can see the cognitive domains that sex helps predict cognitive domains in our sample.

To reliably ascertain the effect of sex on cognition we should need to adjust for factors that may be associated with sex in our sample and that are affecting cognition.
This is however out of this analysis, which is to build a function that predicts cognition in our sample, whatever is driving this association.

Next thing we may want to include are brain structural measurements such as the thickness of the cortex in across different regions.
This is an obvious candidate.
By having included age and sex in the function, the additional variability added by brain regions will be unrelated to these.
This is a design decision.
We may want to find the regions that are related to cognition mediated by age, but this is not the question we are asking here.
Here we want to find the factors that predict cognition.
Since we have included age and sex in the model, what we are interested in brain is whatever it explains that is not already explained by these.

Since there are lot of variables in the brain (>50), we show below an animation by including one variable at a time (starting by age and sex), to see how the variance is gradually explained.

![](/images/blog/2019-03-03-basics-on-learning-brain-cognitive-correlates/lars_anim.gif)

As we include more variables the fit gets better until eventually the predictions in our sample are perfect.
We could now look at the coefficients to deduce importance of regions.

Does this mean that we can predict now perfectly the cognitive performance when given age, sex and cortical thickness of any individual.
Obviously, this is not true because even the same people may perform differently if repeating.
Therefore, we may not aspire at doing it perfect.

Let us see how the model fitted in the previous sample performs in another sample of people from the same population:

![](/images/blog/2019-03-03-basics-on-learning-brain-cognitive-correlates/plot_oos.png)

The errors are so large that do not even fit in the plot.
Therefore, here we cannot extract any conclusion that generalizes.
What has happened ?
With so many variables, we have allowed too much flexibility that the model adapts perfectly to the sample but tells nothing about the population.
This is known as [overfitting](https://en.wikipedia.org/wiki/Overfitting).

When using many variables, if we want to extract valid conclusions about data it needs to generalize.
With few variables this is not a problem and we may reach valid conclusions that generalize.
This is the approach taken when there is a clear hypothesis and when we have the right measurements to validate the hypothesis.
However, when we have a lot of variables and want to extract valid conclusions, prediction is one way to go.
We could also do multiple individual analyses, including only one variable (keeping confounders fixed throughout).
This is the approach taken in voxel-based morphometry (in neuroimage) or the famous [GWAS](https://en.wikipedia.org/wiki/Genome-wide_association_study).
In this way we may extract conclusions about each individual entity.
Prediction allows us to account for the joint effect

By aspiring to explain perfectly we fit the idiosincracies of the particular sample that will vary with another sample.

We want a model that explains the relationships in our data as accurately as possible.
We also want a model that tells something about the population.
It just happens that, when we have many explanatory variables, when we try to fit the data, we are trading-off generalization.
That is, the model becomes specific to the sample and therefore varies when changing sample, which contradicts the fact that is generalizable.
Therefore, we need to find a trade-off between accuracy in fitting the data and constancy of the model when fitted on another sub-set.
In machine learning theory, this is known as the [bias-variance tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff).

It may seem counter-intuitive to sacrifice fitting, ie, to introduce errors on purpose.
As an argument for this, you can think of the fact that it is not reasonable to aspire to explain the data perfectly for the following reasons:
- random errors: the same person may perform differently in the same test for no specific causes 
- wrong model assumptions: the linear model proposed may not be the correct one to explain the phenomenon
- missing factors: we may be not measuring factors that are influencing the cognitive performance

We sacrifice fitting by making models simpler and as consequence we aspire to capture a relationship that generalizes.

One way to do this is to restrict the model to only use a reduced set of variables instead of all.
This is the approach followed by best-feature-subset methods such as the [lasso](https://en.wikipedia.org/wiki/Lasso_(statistics)).

How do we decide on the appropriate complexity ?
This is the topic of [model selection](https://en.wikipedia.org/wiki/Model_selection).
One of the most common is [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)), which consists in helding-out part of the data for measuring generalizability.
 
Let's do this.
- We partition in 3 sets train, validation and test.
- In the training set we fit different versions of the model, by varying the degree of complexity.
- In the validation set we measure prediction accuracy, ie, how well the model fitted on training predicts cognition on the validation set ?
- We select the complexity level that best predicts on the validation
- We train on the whole data (training and validation) with the chosen complexity level

Now we can test the model on the test set, which has not been used before, and which represents new data that may come in the future.
The image below shows the prediction on a test set using age, sex and all brain variables.

![](/images/blog/2019-03-03-basics-on-learning-brain-cognitive-correlates/plot_lasso.png)

Let's see what brain structures got selected by lasso as important for prediction of cognition (and the magnitude of the importance).


![](/images/blog/2019-03-03-basics-on-learning-brain-cognitive-correlates/ef_dorsal.png)
![](/images/blog/2019-03-03-basics-on-learning-brain-cognitive-correlates/evm_dorsal.png)
![](/images/blog/2019-03-03-basics-on-learning-brain-cognitive-correlates/ps_dorsal.png)
![](/images/blog/2019-03-03-basics-on-learning-brain-cognitive-correlates/wm_dorsal.png)



